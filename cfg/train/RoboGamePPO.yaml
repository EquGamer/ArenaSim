params:
  seed: ${...seed}
  algo:
    name: sp # dusp, dusp_5, dusp_8, pfsp, ppo, mappo
  model:
    name: multi_discrete_a2c
  network:
    name: actor_critic
    separate: False
    space:
      multi_discrete:
    mlp:
      units: [256, 128] 
      activation: relu 
      initializer:
        name: default
      regularizer:
        name: None

  enableMultiAgent: ${eq:${.algo.name},"mappo"}
  update_win_rate: 0.55  
  player_pool_length: 1000 
  games_to_check: 320 
  min_eval_games: 127 

  config:
    name: ${resolve_default:RoboGame,${....experiment}}
    reward_shaper:
      scale_value: 1
    normalize_advantage: True
    gamma: 0.99
    tau: 0.95
    learning_rate: 5e-4 
    score_to_win: 20000 
    grad_norm: 0.5
    entropy_coef: 0.001
    truncate_grads: True
    e_clip: 0.2 
    clip_value: True 
    num_actors: ${....task.env.numEnvs}
    horizon_length: 16 
    minibatch_size: ${multiply:${....task.env.numEnvs},8} 
    mini_epochs: 8
    critic_coef: 1
    lr_schedule: None
    kl_threshold: 0.05 
    normalize_input: True
    normalize_value: True
    use_action_masks: True
    ignore_dead_batches : False
    max_epochs: ${resolve_default:1000,${....max_iterations}} 
    seq_len: 4
    bounds_loss_coef: 0.0001 
    full_experiment_name: ${.name}
    device: ${....rl_device}
    device_name: ${....rl_device}
    env_name: rlgpu
    ppo: True
    save_best_after: 40 
    save_frequency: 50

